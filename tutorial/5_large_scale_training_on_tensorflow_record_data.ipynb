{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XDoIbsKzURJq","outputId":"de65d3ae-9c8e-4773-c930-923a4c231069","executionInfo":{"status":"ok","timestamp":1651616506933,"user_tz":420,"elapsed":17409,"user":{"displayName":"Shaowu Pan","userId":"08890366980277406906"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/pswpswpsw/nif.git\n","  Cloning https://github.com/pswpswpsw/nif.git to /tmp/pip-req-build-fcp_ean3\n","  Running command git clone -q https://github.com/pswpswpsw/nif.git /tmp/pip-req-build-fcp_ean3\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from NIF==1.0.0) (3.2.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from NIF==1.0.0) (1.21.6)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->NIF==1.0.0) (1.4.2)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->NIF==1.0.0) (3.0.8)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->NIF==1.0.0) (2.8.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->NIF==1.0.0) (0.11.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->NIF==1.0.0) (4.2.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->NIF==1.0.0) (1.15.0)\n"]}],"source":["!pip install git+https://github.com/pswpswpsw/nif.git"]},{"cell_type":"markdown","metadata":{"id":"3tCKY9sE_mMH"},"source":["# How to training on large dataset (>5GB) where data cannot fit in memory?\n","\n","### Note \n","\n","- GPU memory is precious, don't waste it on dataset\n","- It can be super interesting to train NIF on HUGE dataset!\n","- In this Google Colab, I will show a demonstration using [tensorflow record](https://www.tensorflow.org/tutorials/load_data/tfrecord),\n","which is a simple format for storing a sequence of binary records.\n","- First you will learn how to obtain `tfrecord` file from `npz`, then you will\n","learn how to perform training across different `tfrecord` files."]},{"cell_type":"markdown","metadata":{"id":"eh7PVY82_mMM"},"source":["## 0. Prepare a \"large\" numpy `npz` to play with\n","\n","- this \"ficitious\" dataset has 7 dimensions, \n","$$(t,x,y,z,u,v,w)$$\n","where $u,v,w$ are three components of velocity field. \n","- Thus, we have **4 features, and 3 targets, no weight**. We will need this information to create the dataset \n","\n","- let's call it `Big_npz_file.npz`, it has 54 MB with $10^6$ points, each point contains 7 real number values\n","- it has a key argument `data`, which stores the data\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dqL9GP1M_mMP"},"outputs":[],"source":["import numpy as np\n","\n","data = np.random.uniform(0,1,(1000000,7))\n","np.savez('Big_npz_file.npz', data=data)"]},{"cell_type":"markdown","metadata":{"id":"w6IxHSl8_mMQ"},"source":["## 1. Generate `tfrecord` files from a big a `npz` "]},{"cell_type":"markdown","metadata":{"id":"lWamkgKT_mMS"},"source":["First, you need to bring the `TFRDataset` class under `nif.data.tfr_dataset`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vx__GYjvVbcY","pycharm":{"name":"#%%\n"},"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651616511146,"user_tz":420,"elapsed":3904,"user":{"displayName":"Shaowu Pan","userId":"08890366980277406906"}},"outputId":"746a5eb4-eb25-4993-aee9-74f1ac8be331"},"outputs":[{"output_type":"stream","name":"stdout","text":["1 Physical GPUs, 1 Logical GPUs\n"]}],"source":["from nif.data.tfr_dataset import TFRDataset"]},{"cell_type":"markdown","metadata":{"id":"UbbFVxGO_mMT"},"source":["Second, create an instance, a \"file handler\", call it `fh`, given the `n_features = 3` and `n_target = 3`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"boY13Bt2_mMT"},"outputs":[],"source":["fh = TFRDataset(n_feature=4, n_target=3)"]},{"cell_type":"markdown","metadata":{"id":"O33yWbET_mMV"},"source":["### Then, you need to decide \n","1. How many points to store in one single **tfrecord** file ----> `num_pts_per_file`\n","    - it doesn't have to be perfectly dividing the total number of points, we will make the last one smaller if it doesn't have a perfect divide.\n","    - Tensorflow official document [suggest](https://www.tensorflow.org/tutorials/load_data/tfrecord) around 100 MB.\n","    \n","2. What's the `npz` filepath? ----> `npz_path`\n","3. What's the key to access the 2-D matrix pointwise data? ----> `npz_key`\n","4. Where do you want to put the generated tfrecord file? ----> `tfr_path`\n","5. What's the prefix you want to put in the filename for the tfrecord file? \n","    - they will be something like `prefix_0.npz`, ..., `prefix_10.npz`, ..."]},{"cell_type":"markdown","metadata":{"id":"U_A8tTX8_mMX"},"source":["###  If you have answers to the above in your mind, now you can just call \n","### `fh.create_from_npz(num_pts_per_file, npz_path, npz_key, write_tfr_path, prefix)`"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iBm-fvze_mMX","executionInfo":{"status":"ok","timestamp":1651616514318,"user_tz":420,"elapsed":3187,"user":{"displayName":"Shaowu Pan","userId":"08890366980277406906"}},"outputId":"a262c881-1400-46f7-acad-d987a5a69047"},"outputs":[{"output_type":"stream","name":"stdout","text":["total number of TFR files =  100\n","working in 1-th file... total 100\n","working in 2-th file... total 100\n","working in 3-th file... total 100\n","working in 4-th file... total 100\n","working in 5-th file... total 100\n","working in 6-th file... total 100\n","working in 7-th file... total 100\n","working in 8-th file... total 100\n","working in 9-th file... total 100\n","working in 10-th file... total 100\n","working in 11-th file... total 100\n","working in 12-th file... total 100\n","working in 13-th file... total 100\n","working in 14-th file... total 100\n","working in 15-th file... total 100\n","working in 16-th file... total 100\n","working in 17-th file... total 100\n","working in 18-th file... total 100\n","working in 19-th file... total 100\n","working in 20-th file... total 100\n","working in 21-th file... total 100\n","working in 22-th file... total 100\n","working in 23-th file... total 100\n","working in 24-th file... total 100\n","working in 25-th file... total 100\n","working in 26-th file... total 100\n","working in 27-th file... total 100\n","working in 28-th file... total 100\n","working in 29-th file... total 100\n","working in 30-th file... total 100\n","working in 31-th file... total 100\n","working in 32-th file... total 100\n","working in 33-th file... total 100\n","working in 34-th file... total 100\n","working in 35-th file... total 100\n","working in 36-th file... total 100\n","working in 37-th file... total 100\n","working in 38-th file... total 100\n","working in 39-th file... total 100\n","working in 40-th file... total 100\n","working in 41-th file... total 100\n","working in 42-th file... total 100\n","working in 43-th file... total 100\n","working in 44-th file... total 100\n","working in 45-th file... total 100\n","working in 46-th file... total 100\n","working in 47-th file... total 100\n","working in 48-th file... total 100\n","working in 49-th file... total 100\n","working in 50-th file... total 100\n","working in 51-th file... total 100\n","working in 52-th file... total 100\n","working in 53-th file... total 100\n","working in 54-th file... total 100\n","working in 55-th file... total 100\n","working in 56-th file... total 100\n","working in 57-th file... total 100\n","working in 58-th file... total 100\n","working in 59-th file... total 100\n","working in 60-th file... total 100\n","working in 61-th file... total 100\n","working in 62-th file... total 100\n","working in 63-th file... total 100\n","working in 64-th file... total 100\n","working in 65-th file... total 100\n","working in 66-th file... total 100\n","working in 67-th file... total 100\n","working in 68-th file... total 100\n","working in 69-th file... total 100\n","working in 70-th file... total 100\n","working in 71-th file... total 100\n","working in 72-th file... total 100\n","working in 73-th file... total 100\n","working in 74-th file... total 100\n","working in 75-th file... total 100\n","working in 76-th file... total 100\n","working in 77-th file... total 100\n","working in 78-th file... total 100\n","working in 79-th file... total 100\n","working in 80-th file... total 100\n","working in 81-th file... total 100\n","working in 82-th file... total 100\n","working in 83-th file... total 100\n","working in 84-th file... total 100\n","working in 85-th file... total 100\n","working in 86-th file... total 100\n","working in 87-th file... total 100\n","working in 88-th file... total 100\n","working in 89-th file... total 100\n","working in 90-th file... total 100\n","working in 91-th file... total 100\n","working in 92-th file... total 100\n","working in 93-th file... total 100\n","working in 94-th file... total 100\n","working in 95-th file... total 100\n","working in 96-th file... total 100\n","working in 97-th file... total 100\n","working in 98-th file... total 100\n","working in 99-th file... total 100\n","working in 100-th file... total 100\n"]}],"source":["fh.create_from_npz(num_pts_per_file=1e4, npz_path='Big_npz_file.npz',\n","                   npz_key='data', tfr_path='TFR_dir',\n","                   prefix=\"case1\")"]},{"cell_type":"markdown","metadata":{"id":"57C8yWn9_mMY"},"source":["### Now, you can see the **tfrecord** files that you generated! "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jrQoABfS_mMZ","executionInfo":{"status":"ok","timestamp":1651616514319,"user_tz":420,"elapsed":17,"user":{"displayName":"Shaowu Pan","userId":"08890366980277406906"}},"outputId":"5d6aaa2d-5591-4eb8-ac95-699783a03799"},"outputs":[{"output_type":"stream","name":"stdout","text":["total 27M\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_0.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_1.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_2.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_3.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_4.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_5.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_6.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_7.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_8.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_9.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_10.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_11.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_12.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_13.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_14.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_15.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_16.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_17.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_18.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_19.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_20.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_21.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_22.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_23.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_24.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_25.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_26.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_27.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_28.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_29.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_30.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_31.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_32.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_33.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_34.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_35.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_36.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_37.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_38.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_39.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_40.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_41.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_42.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_43.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_44.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_45.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_46.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_47.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_48.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_49.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_50.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_51.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_52.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_53.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_54.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_55.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_56.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_57.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_58.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_59.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_60.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_61.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_62.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_63.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_64.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_65.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_66.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_67.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_68.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_69.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_70.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_71.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_72.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_73.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_74.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_75.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_76.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_77.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_78.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_79.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_80.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_81.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_82.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_83.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_84.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_85.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_86.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_87.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_88.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_89.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_90.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_91.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_92.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_93.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_94.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_95.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_96.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_97.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_98.tfrecord\n","-rw-r--r-- 1 root root 274K May  3 22:21 case1_99.tfrecord\n"]}],"source":["%ls TFR_dir -lrth"]},{"cell_type":"markdown","metadata":{"id":"DbbT5GhR_mMZ"},"source":["# 2. How to train a Keras Model on the tfrecord files that generated above?"]},{"cell_type":"markdown","metadata":{"id":"sqWGI3nj_mMZ"},"source":["### Let's create a toy Model first"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NsHDx4Or_mMa"},"outputs":[],"source":["import tensorflow as tf \n","\n","x = tf.keras.Input(4,)\n","l1 = tf.keras.layers.Dense(3, activation='tanh')\n","model = tf.keras.Model([x], [l1(x)])\n","model.compile(optimizer='adam', loss='mse')"]},{"cell_type":"markdown","metadata":{"id":"QKcgsISy_mMa"},"source":["Next, we generate a **meta dataset** or maybe you can call it **hyper dataset**. We will do something called **sub-dataset-batching** (I called it). \n","\n","- First you need to determine how many epoch you want to run ----> `epoch`\n","- Then we call the method `.get_tfr_meta_dataset(path, epoch)`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_8Y2yFH__mMa"},"outputs":[],"source":["epoch = 2\n","meta_dataset = fh.get_tfr_meta_dataset(tfr_path='TFR_dir', epoch=epoch)"]},{"cell_type":"markdown","metadata":{"id":"-oyX7bdj_mMb"},"source":["### Finally, we do sub-dataset batching with `meta_dataset`\n","\n","- we obtain `batch_file`, which are Tensors basically, from meta_dataset\n","- we create a `sub` dataset from `batch_file`\n","\n","- you can add your favourite callbacks if you want\n","- the default `epoch` in `model.fit` is 1, which makes sense in this context"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z4-Cve7A_mMb","outputId":"9026d303-a42c-49bd-b71d-b1a910f9ee62"},"outputs":[{"output_type":"stream","name":"stdout","text":["79/79 [==============================] - 1s 3ms/step - loss: 0.1914\n","79/79 [==============================] - 0s 3ms/step - loss: 0.1458\n","79/79 [==============================] - 0s 3ms/step - loss: 0.1288\n","79/79 [==============================] - 0s 5ms/step - loss: 0.1223\n","79/79 [==============================] - 0s 5ms/step - loss: 0.1181\n","79/79 [==============================] - 0s 6ms/step - loss: 0.1153\n","79/79 [==============================] - 0s 6ms/step - loss: 0.1104\n","79/79 [==============================] - 0s 5ms/step - loss: 0.1072\n","79/79 [==============================] - 0s 5ms/step - loss: 0.1050\n","79/79 [==============================] - 0s 5ms/step - loss: 0.1019\n","79/79 [==============================] - 0s 5ms/step - loss: 0.0989\n","79/79 [==============================] - 0s 4ms/step - loss: 0.0973\n","79/79 [==============================] - 0s 5ms/step - loss: 0.0947\n","79/79 [==============================] - 1s 6ms/step - loss: 0.0935\n","79/79 [==============================] - 0s 6ms/step - loss: 0.0912\n","79/79 [==============================] - 0s 6ms/step - loss: 0.0910\n","79/79 [==============================] - 1s 6ms/step - loss: 0.0892\n","79/79 [==============================] - 1s 6ms/step - loss: 0.0886\n","79/79 [==============================] - 1s 6ms/step - loss: 0.0876\n","79/79 [==============================] - 0s 6ms/step - loss: 0.0861\n","79/79 [==============================] - 0s 5ms/step - loss: 0.0869\n","79/79 [==============================] - 0s 5ms/step - loss: 0.0862\n","79/79 [==============================] - 0s 5ms/step - loss: 0.0856\n","79/79 [==============================] - 0s 5ms/step - loss: 0.0851\n","79/79 [==============================] - 1s 7ms/step - loss: 0.0854\n","79/79 [==============================] - 0s 5ms/step - loss: 0.0840\n","79/79 [==============================] - 1s 6ms/step - loss: 0.0850\n","79/79 [==============================] - 0s 6ms/step - loss: 0.0841\n","79/79 [==============================] - 0s 5ms/step - loss: 0.0837\n","79/79 [==============================] - 1s 6ms/step - loss: 0.0842\n","79/79 [==============================] - 0s 5ms/step - loss: 0.0835\n","79/79 [==============================] - 1s 6ms/step - loss: 0.0833\n","79/79 [==============================] - 0s 5ms/step - loss: 0.0837\n","79/79 [==============================] - 0s 5ms/step - loss: 0.0833\n","79/79 [==============================] - 1s 6ms/step - loss: 0.0844\n","79/79 [==============================] - 0s 5ms/step - loss: 0.0840\n","79/79 [==============================] - 0s 5ms/step - loss: 0.0840\n","79/79 [==============================] - 0s 6ms/step - loss: 0.0832\n","79/79 [==============================] - 0s 3ms/step - loss: 0.0833\n","79/79 [==============================] - 0s 3ms/step - loss: 0.0830\n","79/79 [==============================] - 0s 3ms/step - loss: 0.0838\n","79/79 [==============================] - 0s 3ms/step - loss: 0.0831\n","79/79 [==============================] - 0s 3ms/step - loss: 0.0827\n","79/79 [==============================] - 0s 3ms/step - loss: 0.0840\n","79/79 [==============================] - 0s 3ms/step - loss: 0.0827\n","79/79 [==============================] - 0s 3ms/step - loss: 0.0835\n","79/79 [==============================] - 0s 3ms/step - loss: 0.0835\n","79/79 [==============================] - 0s 3ms/step - loss: 0.0834\n","79/79 [==============================] - 0s 3ms/step - loss: 0.0839\n","79/79 [==============================] - 0s 3ms/step - loss: 0.0834\n","79/79 [==============================] - 0s 3ms/step - loss: 0.0833\n","79/79 [==============================] - 0s 3ms/step - loss: 0.0831\n","79/79 [==============================] - 0s 3ms/step - loss: 0.0831\n","79/79 [==============================] - 0s 3ms/step - loss: 0.0834\n","79/79 [==============================] - 0s 3ms/step - loss: 0.0825\n","79/79 [==============================] - 0s 3ms/step - loss: 0.0838\n","79/79 [==============================] - 0s 3ms/step - loss: 0.0827\n","79/79 [==============================] - 0s 3ms/step - loss: 0.0837\n","79/79 [==============================] - 0s 3ms/step - loss: 0.0837\n","79/79 [==============================] - 0s 3ms/step - loss: 0.0834\n"," 1/79 [..............................] - ETA: 1s - loss: 0.0830"]}],"source":["callbacks = []\n","batch_size = 128\n","\n","for batch_file in meta_dataset:\n","    batch_dataset = fh.gen_dataset_from_batch_file(batch_file, batch_size)\n","    model.fit(batch_dataset, verbose='auto', callbacks=callbacks)"]},{"cell_type":"markdown","metadata":{"id":"GAerasqM_mMc"},"source":["### Cautious! \n","- it is not perfect since we are `.fit` for each `tfrecord` and this will make epoch information lost after each recall of `.fit`\n","- we can hack it by using a `callbacks` that explicitly write loss to the disk.. in an append way.\n","- **again what is presented here is not perfect, I am open for your solutions!**"]},{"cell_type":"markdown","metadata":{"id":"cCMnnQ3n_mMc"},"source":["### Question:\n","\n","- **Why are you choosing to chop tfrecord like this?** It seems to me it is not what the official document has suggested, the syntax here looks very not software-optimized. \n","\n","### **Answer:** \n","\n","- Because the standard way of using `tfrecord` will take each data points, i.e., $(t,x,y,z,u,v,w)$ as an `tf.train.Example`. When you have 10 of millions, hundreds of millsion data points, **generating such dataset (not using this dataset for training)** has been reported to have performance issues if your algorithm contains any native Python Loop, which is not avoidable at the current stage. \n","\n","- In `nif.data.tfr_dataset.TFRDataset`, we subsampling npz file in a sequential order and take data along each dimension (e.g., (t_0,...,t_M) ) as an `tf.train.Example`, instead of treating each single point `(t,x,y,z,u,v,w)` as an example. So we have 7 `Example` in this demo. \n","\n","- In this way, performance issue is greatly reduced. I believe the reason is that we leave the inner looping over all datapoints to the low-level\n","\n","- So far, I can generate tfrecord from tens of Gigabyte data within 30 mins to 1 hour. While if I tried to make each training example as data point, it may take \n","\n","### Let me know <shawnpan@uw.edu>  if you have better solution for this, i.e., without using the meta-dataset to do sub-dataset-batching"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"5_large_scale_training_on_tensorflow_record_data.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}